{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('-p', 'C:/Users/Elitebook/Downloads/my_data')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'-p', 'C:/Users/Elitebook/Downloads/my_data'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted hi.translit.sampled.train.tsv to hi_train.csv\n",
            "Converted hi.translit.sampled.dev.tsv to hi_valid.csv\n",
            "Converted hi.translit.sampled.test.tsv to hi_test.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to your downloaded Dakshina Hindi dataset\n",
        "input_folder = r\"C:\\Users\\Elitebook\\nashra Dropbox\\Nashra\\PC\\Downloads\\dakshina_dataset_v1.0\\dakshina_dataset_v1.0\\hi\\lexicons\"\n",
        "output_folder = r\"C:\\Users\\Elitebook\\Downloads\\my_data\"\n",
        "\n",
        "# Ensure output folder exists\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Map from original .tsv file names to desired .csv output file names\n",
        "file_map = {\n",
        "    \"hi.translit.sampled.train.tsv\": \"hi_train.csv\",\n",
        "    \"hi.translit.sampled.dev.tsv\": \"hi_valid.csv\",\n",
        "    \"hi.translit.sampled.test.tsv\": \"hi_test.csv\"\n",
        "}\n",
        "\n",
        "# Convert each file\n",
        "for tsv_file, csv_file in file_map.items():\n",
        "    tsv_path = os.path.join(input_folder, tsv_file)\n",
        "    csv_path = os.path.join(output_folder, csv_file)\n",
        "\n",
        "    # Read TSV and save as CSV\n",
        "    df = pd.read_csv(tsv_path, sep='\\t', header=None)\n",
        "    df.to_csv(csv_path, index=False, header=False)\n",
        "\n",
        "    print(f\"Converted {tsv_file} to {csv_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxckZAM8Zb8k",
        "outputId": "7e4437a0-893e-4f4f-8d18-601ca61240e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.13.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-lightning-2.5.1.post0 torchmetrics-1.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Elitebook\\_netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamaannashra012\u001b[0m (\u001b[33mamaannashra012-iit-madras-alumni-association\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"c32cc9e7b9c39295b3b1821cf5bf4fe508d41045\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "VKeGl_2cZA2o",
        "outputId": "4e57cc4f-62b3-4fa8-c1af-906f16592053"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Elitebook\\nashra Dropbox\\Nashra\\PC\\Downloads\\wandb\\run-20250519_155204-tvat8h0n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/amaannashra012-iit-madras-alumni-association/Assignment3_DL/runs/tvat8h0n' target=\"_blank\">polar-water-1</a></strong> to <a href='https://wandb.ai/amaannashra012-iit-madras-alumni-association/Assignment3_DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/amaannashra012-iit-madras-alumni-association/Assignment3_DL' target=\"_blank\">https://wandb.ai/amaannashra012-iit-madras-alumni-association/Assignment3_DL</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/amaannashra012-iit-madras-alumni-association/Assignment3_DL/runs/tvat8h0n' target=\"_blank\">https://wandb.ai/amaannashra012-iit-madras-alumni-association/Assignment3_DL/runs/tvat8h0n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "c:\\Users\\Elitebook\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
            "\n",
            "  | Name    | Type                   | Params | Mode \n",
            "-----------------------------------------------------------\n",
            "0 | encoder | encoder                | 2.6 M  | train\n",
            "1 | decoder | decoder_with_Attention | 2.1 M  | train\n",
            "-----------------------------------------------------------\n",
            "4.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.8 M     Total params\n",
            "19.160    Total estimated model params size (MB)\n",
            "13        Modules in train mode\n",
            "0         Modules in eval mode\n",
            "c:\\Users\\Elitebook\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
            "c:\\Users\\Elitebook\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 346/346 [11:01<00:00,  0.52it/s, v_num=8h0n, val_loss=0.868, train_loss=1.210]val accuracy is 0.3497016980266177\n",
            "Epoch 1: 100%|██████████| 346/346 [10:08<00:00,  0.57it/s, v_num=8h0n, val_loss=0.809, train_loss=0.582, val_acc=0.350]val accuracy is 0.38710417622762733\n",
            "Epoch 2: 100%|██████████| 346/346 [10:47<00:00,  0.53it/s, v_num=8h0n, val_loss=0.786, train_loss=0.507, val_acc=0.387]val accuracy is 0.4194584671867829\n",
            "Epoch 3: 100%|██████████| 346/346 [14:19<00:00,  0.40it/s, v_num=8h0n, val_loss=0.771, train_loss=0.474, val_acc=0.419]val accuracy is 0.4442404772831574\n",
            "Epoch 4: 100%|██████████| 346/346 [14:53<00:00,  0.39it/s, v_num=8h0n, val_loss=0.766, train_loss=0.448, val_acc=0.444]val accuracy is 0.4472234970169803\n",
            "Epoch 5: 100%|██████████| 346/346 [12:00<00:00,  0.48it/s, v_num=8h0n, val_loss=0.745, train_loss=0.427, val_acc=0.447]val accuracy is 0.4378155117026159\n",
            "Epoch 6: 100%|██████████| 346/346 [10:09<00:00,  0.57it/s, v_num=8h0n, val_loss=0.796, train_loss=0.415, val_acc=0.438]val accuracy is 0.45089490592014686\n",
            "Epoch 7: 100%|██████████| 346/346 [13:48<00:00,  0.42it/s, v_num=8h0n, val_loss=0.752, train_loss=0.402, val_acc=0.451]val accuracy is 0.44607618173474073\n",
            "Epoch 8: 100%|██████████| 346/346 [14:02<00:00,  0.41it/s, v_num=8h0n, val_loss=0.774, train_loss=0.392, val_acc=0.446]val accuracy is 0.4550252409362093\n",
            "Epoch 9: 100%|██████████| 346/346 [10:44<00:00,  0.54it/s, v_num=8h0n, val_loss=0.787, train_loss=0.383, val_acc=0.455]val accuracy is 0.44882973841211565\n",
            "Epoch 10: 100%|██████████| 346/346 [10:20<00:00,  0.56it/s, v_num=8h0n, val_loss=0.800, train_loss=0.374, val_acc=0.449]val accuracy is 0.45479577787976133\n",
            "Epoch 11: 100%|██████████| 346/346 [10:30<00:00,  0.55it/s, v_num=8h0n, val_loss=0.763, train_loss=0.366, val_acc=0.455]val accuracy is 0.4577787976135842\n",
            "Epoch 12: 100%|██████████| 346/346 [10:10<00:00,  0.57it/s, v_num=8h0n, val_loss=0.773, train_loss=0.358, val_acc=0.458]val accuracy is 0.45318953648462595\n",
            "Epoch 13: 100%|██████████| 346/346 [10:09<00:00,  0.57it/s, v_num=8h0n, val_loss=0.780, train_loss=0.362, val_acc=0.453]val accuracy is 0.45663148233134465\n",
            "Epoch 14: 100%|██████████| 346/346 [10:13<00:00,  0.56it/s, v_num=8h0n, val_loss=0.779, train_loss=0.355, val_acc=0.457]val accuracy is 0.4598439651216154\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|██████████| 346/346 [10:13<00:00,  0.56it/s, v_num=8h0n, val_loss=0.779, train_loss=0.355, val_acc=0.457]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Restoring states from the checkpoint path at .\\Assignment3_DL\\tvat8h0n\\checkpoints\\epoch=14-step=5190.ckpt\n",
            "Loaded model weights from the checkpoint at .\\Assignment3_DL\\tvat8h0n\\checkpoints\\epoch=14-step=5190.ckpt\n",
            "c:\\Users\\Elitebook\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0: 100%|██████████| 36/36 [00:14<00:00,  2.41it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "      test_accuracy         0.07877795398235321\n",
            "        test_loss            0.773327648639679\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as L\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import csv\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import wandb\n",
        "import argparse\n",
        "from Accesories_functions import trainDataset,Helper_functions\n",
        "from Encoder import encoder\n",
        "from Vanila_decoder import decoder\n",
        "from Encoder_Decoder_vanilla_seq import eng_ben\n",
        "from Attention_Decoder import decoder_with_Attention\n",
        "from seq_to_seq_attention import eng_ben_attention\n",
        "wandb.login()\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')  # Get the GPU device\n",
        "else:\n",
        "    device = torch.device('cpu')   # Use CPU if GPU is not available\n",
        "\n",
        "def main(args):\n",
        "    if 145==145:\n",
        "        obj=Helper_functions()\n",
        "        #config=wandb.config\n",
        "        #wandb.run.name='bs-'+str(config.batch_size)+'-ct-'+ str(config.cell_type)+'-ep-'+str(config.epochs)+ '-es-'+str(config.embedding_size)+'-hs-'+str(config.hidden_size)+'-nel-'+str(config.encoder_num_layers)+'-ndl-'+str(config.decoder_num_layers)+'-do-'+str(config.drop_out)+'-bd-'+str(config.bidirectional)\n",
        "        path=args.path\n",
        "        # Extracting and preprocessing training, validation, and test data\n",
        "        train_data = obj.Extract_data(os.path.join(path, 'hi_train.csv'))\n",
        "        train_test_data = obj.Extract_data(os.path.join(path, 'hi_test.csv'))\n",
        "        train_val_data = obj.Extract_data(os.path.join(path, 'hi_valid.csv'))\n",
        "        encoder_input_text,decoder_input_text,encoder_char_to_index,decoder_char_to_index,encoder_vocab,decoder_vocab=obj.helper(train_data)\n",
        "        # Creating character-to-index and index-to-character mappings for encoder and decoder\n",
        "        decoder_index_to_char={index: char for char, index in decoder_char_to_index.items()}\n",
        "        encoder_index_to_char={index: char for char, index in encoder_char_to_index.items()}\n",
        "        batch_size=args.batch_size\n",
        "        # Creating training, validation, and test datasets and dataloaders\n",
        "        p,p1=obj.words_to_tensor(encoder_input_text,encoder_char_to_index,decoder_input_text,decoder_char_to_index)\n",
        "        dataset1=trainDataset(p,p1)\n",
        "        dataloader=DataLoader(dataset=dataset1,batch_size=batch_size,shuffle=True,num_workers=1)\n",
        "        encoder_input_text_test,decoder_input_text_test,_,_,_,_=obj.helper(train_test_data)\n",
        "        encoder_input_text_val,decoder_input_text_val,_,_,_,_=obj.helper(train_val_data)\n",
        "        p_test,p1_test=obj.words_to_tensor(encoder_input_text_test,encoder_char_to_index,decoder_input_text_test,decoder_char_to_index)\n",
        "        testdataset=trainDataset(p_test,p1_test)  #create train dataset\n",
        "        testDataloader=DataLoader(dataset=testdataset,batch_size=batch_size,shuffle=False,num_workers=1)\n",
        "        p_val,p1_val=obj.words_to_tensor(encoder_input_text_val,encoder_char_to_index,decoder_input_text_val,decoder_char_to_index)\n",
        "        val_dataset=trainDataset(p_val,p1_val)\n",
        "        valDataloader=DataLoader(dataset=val_dataset,batch_size=batch_size,shuffle=False,num_workers=1)\n",
        "        heatMap_dataloader=DataLoader(dataset=testdataset,batch_size=9,shuffle=True,num_workers=1)\n",
        "        first_batch = next(iter(heatMap_dataloader))\n",
        "         # Retrieving model parameters from command-line arguments\n",
        "        cell_type=args.cell_type\n",
        "        embedding_size=args.embedding_size\n",
        "        hidden_size=args.hidden_size\n",
        "        encoder_num_layer=args.encoder_num_layer\n",
        "        decoder_num_layer=args.decoder_num_layer\n",
        "        drop_out=args.dropout_rate\n",
        "        epoch=args.epochs\n",
        "        bidirectional=args.bidirectional\n",
        "        max_seq_length=p.shape[1]\n",
        "        # Initializing Wandb\n",
        "        wandb.init(entity=args.wandb_entity,project=args.project_name)\n",
        "        wandb_logger = WandbLogger(project=args.project_name, entity=args.wandb_entity)\n",
        "        # Initializing encoder and decoder models\n",
        "        en=encoder(cell_type,encoder_vocab,embedding_size,hidden_size,encoder_num_layer,drop_out,bidirectional)\n",
        "        de=decoder(cell_type,decoder_vocab,embedding_size,hidden_size,decoder_num_layer,drop_out,epoch,0.5,bidirectional)\n",
        "        de_att=decoder_with_Attention(max_seq_length,cell_type,decoder_vocab,embedding_size,hidden_size,decoder_num_layer,drop_out,epoch,0.5,bidirectional)\n",
        "         # Selecting the model based on attention flag\n",
        "        if args.attention==False:\n",
        "            model=eng_ben(en,de,decoder_vocab,decoder_index_to_char,encoder_index_to_char)\n",
        "        else:\n",
        "            model=eng_ben_attention(en,de_att,decoder_vocab,decoder_index_to_char,encoder_index_to_char)\n",
        "\n",
        "         # Training the model\n",
        "        trainer = L.Trainer(accelerator='auto',devices=\"auto\",max_epochs=epoch,num_sanity_val_steps=0,logger=wandb_logger)\n",
        "        trainer.fit(model,dataloader,valDataloader)\n",
        "        trainer.test(dataloaders=testDataloader,ckpt_path='best')  # Testing the model\n",
        "        if args.attention==False and args.plot_heatmap==True:  # Printing a message if heatmap plotting is requested but attention is not applied\n",
        "            print(\"attention has not applied to the model\")\n",
        "        elif args.attention==True and args.plot_heatmap==True:\n",
        "            trainer.predict(dataloaders=heatMap_dataloader,ckpt_path='best')\n",
        "\n",
        "if  __name__ ==\"__main__\":\n",
        "    parser = argparse.ArgumentParser()  #taking arguments from command line arguments\n",
        "    # Adding command-line arguments\n",
        "    parser.add_argument('-wp','--project_name',type=str,default='Assignment3_DL',help='Project name used to track experiments in Weights & Biases dashboard')\n",
        "    parser.add_argument('-we','--wandb_entity',type=str,default='amaannashra012-iit-madras-alumni-association',help='Project name used to track experiments in Weights & Biases dashboard')\n",
        "    parser.add_argument('-p','--path',type=str,help='provide the path where your data is stored in memory,Read the readme for more description')\n",
        "    parser.add_argument('-e','--epochs',type=int,default=15,help='Number of epochs to train')\n",
        "    parser.add_argument('-b','--batch_size',type=int,default=128,help='Batch size used to train RNN')\n",
        "    parser.add_argument('-es','--embedding_size',type=int,default=256,help='default size of Enbedding Layer')\n",
        "    parser.add_argument('-hs','--hidden_size',type=int,default=256,help='default size of context vector')\n",
        "    parser.add_argument('-enl','--encoder_num_layer',type=int,default=2,help='number of encoder Layer')\n",
        "    parser.add_argument('-dnl','--decoder_num_layer',type=int,default=1,help='number of decoder Layer')\n",
        "    parser.add_argument('-ct','--cell_type',type=str,default='LSTM',choices=['GRU','RNN','LSTM'],help='which cell gonna use either GRU or LSTM or RNN')\n",
        "    parser.add_argument('-do','--dropout_rate',type=float,default=0.3,choices=[0.2,0.3,0.4],help='drop out rate to regularilze the model')\n",
        "    parser.add_argument('-bd','--bidirectional',type=bool,default=True,choices=[True,False],help='biderctional model or not')\n",
        "    parser.add_argument('-a','--attention',type=int,default=1,choices=[1,0],help='We will use attention or not')\n",
        "    parser.add_argument('-hp','--plot_heatmap',type=int,default=0,choices=[1,0],help='We will plot the heatmap or not')\n",
        "    args = parser.parse_args(args=['-p', 'C:/Users/Elitebook/Downloads/my_data'])\n",
        "\n",
        "    main(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question number 6 code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_attention_matrix(input_seq, output_seq, attention_weights, title=\"Attention Heatmap\"):\n",
        "    \"\"\"\n",
        "    Plots an attention heatmap between input and output sequences.\n",
        "\n",
        "    Args:\n",
        "        input_seq (list): List of input characters (e.g., ['p', 'a', 'n', 'd', 'a']).\n",
        "        output_seq (list): List of predicted output characters (e.g., ['ప', 'ా', 'ం']).\n",
        "        attention_weights (Tensor or np.ndarray): Attention weights of shape (output_len, input_len).\n",
        "        title (str): Title of the plot.\n",
        "    \"\"\"\n",
        "    if isinstance(attention_weights, torch.Tensor):\n",
        "        attention_weights = attention_weights.detach().cpu().numpy()\n",
        "    \n",
        "    \n",
        "    plt.figure(figsize=(len(input_seq), len(output_seq)))\n",
        "    sns.heatmap(attention_weights, xticklabels=input_seq, yticklabels=output_seq, cmap=\"viridis\", annot=True, fmt=\".2f\")\n",
        "    plt.xlabel(\"Input Sequence\")\n",
        "    plt.ylabel(\"Output Sequence\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
